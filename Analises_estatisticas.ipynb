{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fcc777f-3efa-43db-a6bd-ac38b0eee910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Biblioteca para tratativa\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import re \n",
    "from collections import Counter ## Para descobrir número de repetição de um valor em uma lista\n",
    "\n",
    "\n",
    "# Bibliotecas para classe Normalidade\n",
    "import seaborn as sns ## Para plotar gráficos \n",
    "import numpy as np ## Para calculos matemáticos\n",
    "import matplotlib.pyplot as plt ## Para plot de gráficos\n",
    "from sklearn.preprocessing import MinMaxScaler ## Para padronizar os dados em reescala de Max e Min \n",
    "from statsmodels.graphics.gofplots import qqplot ## Para plot de gráficos\n",
    "\n",
    "# Bibliotecas para estatística\n",
    "import scipy.stats ## Classe com funções estatísticas\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo ## Para calculo de KMO e MSA\n",
    "import researchpy as rs ## Para realizar a correlação entre as variáveis \n",
    "\n",
    "\n",
    "'''\n",
    "    About this file:\n",
    "    https://miro.com/app/board/uXjVPiH_KCg=/?share_link_id=298618246717\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class Tratativa:\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    # Função: conhecimento_base\n",
    "    #\n",
    "    # Descrição: Realiza a retirada de algumas informações da base passada, alocando-os em um dataframe,\n",
    "    #            explicando assim cada coluna independentemente\n",
    "    #\n",
    "    # Parâmetros: 1) dataframe = dataframe inicial com os dados\n",
    "    #\n",
    "    # Quem procurar: Otávio Augusto Iasbeck\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    def conhecimento_base(dataframe) -> pd.DataFrame:\n",
    "\n",
    "        ''' \n",
    "\n",
    "            Parâmetros:\n",
    "                1) Dataframe\n",
    "\n",
    "            Retorno:\n",
    "                Dataframe com métricas de tendencia central e outros detalhes \n",
    "\n",
    "        '''\n",
    "\n",
    "        ## Crio dataframe passando as colunas como coluna, em uma coluna chamada \"coluna\" kk\n",
    "        df_html = pd.DataFrame(\n",
    "            columns=['column'],\n",
    "            data=[\n",
    "                [coluna]\n",
    "                for coluna\n",
    "                in dataframe.columns\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        ## Trago o shape (qtd linhas e qtd colunas)\n",
    "        df_shape = dataframe.shape\n",
    "    \n",
    "\n",
    "    ## A partir daqui, vou criando colunas e trazendo métricas através do método describe(), most_common, Counter entre outros\n",
    "\n",
    "        ## Pego o tipo de cada coluna\n",
    "        df_html['types'] = df_html.apply(\n",
    "            lambda x: dataframe.dtypes[x['column']],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        ## Pego alguns exemplos de dados que o atributo possui (5 primeiros)\n",
    "        df_html['samples'] = df_html.apply(\n",
    "            lambda x: list(dataframe[x['column']][:5]),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        ## Trago os valores que mais se repetem em cada atributo \n",
    "        df_html['most common'] = df_html.apply(\n",
    "            lambda x: '{0} -> {1}% - [{2}]'.format(\n",
    "                    Counter(list(dataframe[x['column']])).most_common(1)[0][0],\n",
    "                    round((Counter(list(dataframe[x['column']])).most_common(1)[0][1] / df_shape[0]) * 100, 1),\n",
    "                    Counter(list(dataframe[x['column']])).most_common(1)[0][1],\n",
    "                ),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        ## Pego do menor ao maior valor, caso for de tipo 'int' ou 'float'\n",
    "        df_html['ranges'] = df_html.apply(\n",
    "            lambda x: '{} até {}'.format(\n",
    "                dataframe[x['column']].min(), dataframe[x['column']].max())\n",
    "            if 'int' in str(x['types']) or 'float' in str(x['types'])\n",
    "            else '',\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        ## Trago primeiro quartil, caso for de tipo 'int' ou 'float'\n",
    "        df_html['q1'] = df_html.apply(\n",
    "            lambda x: dataframe[x['column']].describe()[4]\n",
    "            if 'int' in str(x['types']) or 'float' in str(x['types'])\n",
    "            else '',\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        ## Trago segundo quartil, caso for de tipo 'int' ou 'float'\n",
    "        df_html['q2'] = df_html.apply(\n",
    "            lambda x: dataframe[x['column']].describe()[5]\n",
    "            if 'int' in str(x['types']) or 'float' in str(x['types'])\n",
    "            else '',\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        ## Trago terceiro quartil, caso for de tipo 'int' ou 'float'\n",
    "        df_html['q3'] = df_html.apply(\n",
    "            lambda x: dataframe[x['column']].describe()[6]\n",
    "            if 'int' in str(x['types']) or 'float' in str(x['types'])\n",
    "            else '',\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        ## Pego a média, caso for de tipo 'int' ou 'float'\n",
    "        df_html['mean'] = df_html.apply(\n",
    "            lambda x: round(dataframe[x['column']].describe()[1], 1)\n",
    "            if 'int' in str(x['types']) or 'float' in str(x['types'])\n",
    "            else '',\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        ## Pego o desvio padrão, caso for de tipo 'int' ou 'float'\n",
    "        df_html['std'] = df_html.apply(\n",
    "            lambda x: dataframe[x['column']].describe()[2]\n",
    "            if 'int' in str(x['types']) or 'float' in str(x['types'])\n",
    "            else '',\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        ## Trago quantidade de valores presentes\n",
    "        df_html['count'] = df_html.apply(\n",
    "            lambda x: int(dataframe[x['column']].describe()[0]),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        ## Trago quantidade de valores ausentes\n",
    "        df_html['missings'] = df_html.apply(\n",
    "            lambda x: dataframe[x['column']].isnull().sum(),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        ## Trago quantidade de valores que não se repetem\n",
    "        df_html['uniques'] = df_html.apply(\n",
    "            lambda x: dataframe[x['column']].nunique(),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        return df_html\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    # Função: higienizacao\n",
    "    #\n",
    "    # Descrição: Realiza tratativa de input dos dados, abordando desde a desceleção de colunas, reposicionamento de coluna\\\n",
    "    #            e tentativa de reconhecimento de colunas 'id's'\n",
    "    #\n",
    "    # Parâmetros: 1) dataframe = dataframe inicial com os dados\n",
    "    #\n",
    "    # Quem procurar: Otávio Augusto Iasbeck\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    def higienizacao(df):\n",
    "        \n",
    "        ''' \n",
    "\n",
    "            Parâmetros:\n",
    "                1) Dataframe\n",
    "\n",
    "            Retorno:\n",
    "                Dataframe com métricas de tendencia central e outros detalhes.\n",
    "\n",
    "            # OBSERVAÇÃO:\n",
    "                A higienização descrita foi voltada para um conjunto de dados que variam, mas que se sabe préviamente o que poderá vir\\n\n",
    "                podendo assim não servir para outros tipos de datasets (não excluindo a utilização das classes abaixo!)\n",
    "\n",
    "        '''\n",
    "        \n",
    "        ## Caso haja valores com 'Nan', trocamos por 0 \n",
    "        df.replace(np.nan, 0, inplace = True)\n",
    "        \n",
    "        ## Caso haja uma linha com todos valores zerados, descartamo-as\n",
    "        df = df.loc[(df!=0).any(axis=1)]\n",
    "\n",
    "        ## Caso haja coluna com todos os valores zerados, rastreio-os\n",
    "        lista_colunas_zeradas = [col for col in (df.columns) if (df[col] == 0).all()]\n",
    "                \n",
    "        ## Deleto-os\n",
    "        for i in lista_colunas_zeradas:\n",
    "            df.drop(columns = {f'{i}'}, inplace = True)\n",
    "            print(f\"A coluna {i} foi deletada pois haviam apenas valores zerados nela!\")\n",
    "            \n",
    "        ## Verifico se há uma coluna de Unnamed para deletar\n",
    "        try:\n",
    "            df.drop(columns = {'Unnamed: 0'}, inplace = True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        ## Tentaremos transformar as colunas em 'float'\n",
    "        for coluna in df.columns:\n",
    "            try:\n",
    "                \n",
    "                if ('cod' in coluna.lower()) or ('mes' in coluna.lower()) or ('dia' == coluna.lower()):\n",
    "                    df[coluna] = df[coluna].astype('str')\n",
    "                    \n",
    "                else:    \n",
    "                    df[coluna] = df[coluna].astype('float')\n",
    "                    \n",
    "            except:\n",
    "                \n",
    "                df[coluna] = df[coluna].astype('str')\n",
    "                \n",
    "            ## Removo colunas que aparecem com planejado\n",
    "            if ('plan' in coluna.lower()) or ('_plan' in coluna.lower()):\n",
    "                df.drop(columns = {f'{coluna}'}, inplace = True)\n",
    "                \n",
    "                ## Removo colunas que aparecem com percentuais\n",
    "            if ('perc' in coluna.lower()) or ('perc_' in coluna.lower()):\n",
    "                df.drop(columns = {f'{coluna}'}, inplace = True)\n",
    "            \n",
    "            \n",
    "            ## Removo colunas que aparecem com dummy\n",
    "            if ('dummy' in coluna.lower()) or ('_dummy' in coluna.lower()):\n",
    "                df.drop(columns = {f'{coluna}'}, inplace = True)\n",
    "                \n",
    "                \n",
    "             ## (RECEPTIVO) Se achar coluna de NS_real, coloca como última coluna (excluo ela e coloco de novo, famosa gambs)\n",
    "            if ('ns' == coluna.lower()) or ('servico' in coluna.lower()):\n",
    "                coluna_list = df[coluna]\n",
    "                df.drop(columns = {f'{coluna}'}, inplace = True)\n",
    "                df['NS'] = coluna_list\n",
    "\n",
    "\n",
    "            ## Acho o dia em um datetime por expressão irregular\n",
    "            if ('data' in coluna.lower()) or ('DAT_INSUMO' == coluna.lower()) or ('dat_' in coluna.lower()) or ('dia' == coluna.lower()):\n",
    "                \n",
    "                try:\n",
    "                    if ('t' in df[coluna].iloc[1]):\n",
    "                        ## Se o formato for YYYY/mm/ddTHH:MM:SS retiro o dia da data e aloco em uma coluna\n",
    "                        df[coluna] = [df[coluna].iloc[x][:10] for x in range(len(df[coluna]))]\n",
    "\n",
    "                        ## Acho o dd no formato YYYY/mm/dd\n",
    "                        dias = [re.findall(r'\\d{2}$', x)[0] for x in df[coluna]]\n",
    "\n",
    "                    else:\n",
    "                        try:\n",
    "                            ## Para YYYY-mm-dd\n",
    "                            dias = [re.findall('-\\d{2} ', x)[0][1:-1] for x in df[coluna]]\n",
    "\n",
    "                        except:\n",
    "\n",
    "                            try:\n",
    "                                ## Para tipo dd/mm/YYYY\n",
    "                                \n",
    "                                dias = [re.findall('\\d{2}/', x)[0] for x in df[coluna]]\n",
    "\n",
    "                            except:\n",
    "            \n",
    "                                # Para tipo YYYYmmdd\n",
    "                                dias = [re.findall(r'\\d{2}$', x)[0] for x in df[coluna]]\n",
    "                                \n",
    "                except Exception as erro:\n",
    "                    print(f\"Não foi possível tratar a data do arquivo, vide erro: {erro}\")\n",
    "\n",
    "                \n",
    "                dias = [int(x) for x in dias]\n",
    "                \n",
    "                df['DIAS'] = dias\n",
    "                    \n",
    "            \n",
    "        ## Pegaremos as colunas que conseguimos transformar em 'float' ou que já são naturalmente 'int'\n",
    "        df_continuo = df.select_dtypes([int, float])\n",
    "        \n",
    "        ## Retornaremos o dataframe transformado e o dataframe selecionado\n",
    "        return df, df_continuo\n",
    "    \n",
    "    \n",
    "    \n",
    "class Normalidade:\n",
    "\n",
    "    \n",
    "    class Analisa_normalidade:\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        # Função: distribuicao_grafica\n",
    "        #\n",
    "        # Descrição: Realiza 3 tipos de gráficos para reconhecimento da distribuição visual dos dados. É necessário\\\n",
    "        #            inputar apenas dados de indicadores\n",
    "        #\n",
    "        # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "        #\n",
    "        # Quem procurar: Otávio Augusto Iasbeck\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        def distribuicao_grafica(df, y = None) -> None:\n",
    "\n",
    "            '''\n",
    "                Parâmetro: \n",
    "                    1) df: Dataset com atributos\n",
    "\n",
    "                Retorno:\n",
    "                    1) Printa 3 gráficos para acompanhamento da distribuição dos dados de cada coluna\n",
    "\n",
    "                #### Observações:\n",
    "                    É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "            '''\n",
    "\n",
    "            ## Coloco os dados em ordem crescente, me baseo pela segunda coluna\n",
    "            df.sort_values(by = f'{df.columns[1]}', ascending = True, inplace = True)\n",
    "\n",
    "        ## Realiza plot da distribuição dos dados de cada coluna do dataframe        \n",
    "            for i, col in enumerate(df.columns):\n",
    "\n",
    "                try:\n",
    "                    ## Cria tamanho da figura para cada bloco de exibição\n",
    "                    plt.figure(i, figsize=(30, 20))\n",
    "                    ## Cria quantos blocos de exibição terão um plot\n",
    "                    axes = plt.subplots(1, 3)[1]\n",
    "                    ## Chama gráfico de histograma e o aloca no primeiro \"axes\"\n",
    "                    sns.histplot(y = y, x=col, data=df, ax = axes[0])\n",
    "                    ## Chama gráfico de boxplot e o aloca no segundo \"axes\"\n",
    "                    sns.boxplot(y = y, x=col, data=df, ax = axes[1])\n",
    "                    ## Chama gráfico de disperção e o aloca no terceiro \"axes\"\n",
    "                    qqplot(df[col], line='s', ax=axes[2])\n",
    "\n",
    "                except Exception as erro:\n",
    "                    print(f\"A coluna {col} não pode ser graficada, segue erro: {erro}\")\n",
    "                    continue\n",
    "\n",
    "        \n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        # Função: teste_d_agostinho\n",
    "        #\n",
    "        # Descrição: Utiliza da curtose e assimetria da distribuição dos dados para realizar calculo testando hipótese\\\n",
    "        #            nula, dizendo que os dados provém de distribuiçãi normal \n",
    "        #\n",
    "        # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "        #\n",
    "        # Quem procurar: Otávio Augusto Iasbeck\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        def teste_d_agostinho(df) -> pd.DataFrame:\n",
    "\n",
    "            '''\n",
    "                Parâmetro: \n",
    "                    1) df: Dataset com atributos\n",
    "\n",
    "                Retorno:\n",
    "                    1) Dataset com as colunas de input como valores, com atributos de p_value_d_agostinho, teste_estatistico_d_agostinho, para cada uma delas\n",
    "\n",
    "                #### Observações:\n",
    "                    É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "            '''\n",
    "\n",
    "            p_value_col = []\n",
    "            teste_estatistico = []\n",
    "\n",
    "            ## Percorro colunas\n",
    "            for coluna in df.columns:\n",
    "                try:\n",
    "                    ## Realizo o calculo para cada atributo\n",
    "                    valor_estatistico, p_value = scipy.stats.normaltest(df[coluna])\n",
    "                except Exception as erro:\n",
    "                    ## Se não conseguir realizar o calculo, só aloca a coluna no dataframe\n",
    "                    print(f\"Não pode ser realizado o calculo para a coluna {coluna}, provavelmente por tipagem de dados, segue erro: {erro}\")\n",
    "                \n",
    "                ## Armazeno os valores provindos do método\n",
    "                p_value_col.append(np.around(p_value, 3))\n",
    "                teste_estatistico.append(valor_estatistico)\n",
    "\n",
    "            ## Crio dataframe para alocar cada valor referente à cada atributo, que estão em index\n",
    "            df_variancas = pd.DataFrame(data = {'p_value_d_agostinho':p_value_col, 'atributos':df.columns, 'teste_estatistico_d_agostinho':teste_estatistico})\n",
    "            df_variancas = df_variancas.set_index('atributos') \n",
    "\n",
    "            return df_variancas\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        # Função: teste_shapiro_wilks\n",
    "        #\n",
    "        # Descrição: Realiza o teste de cada coluna para/com hipótese nula de distribuição normal\\\n",
    "        #            retornando um dataframe com o teste estatístico e p-value\n",
    "        #\n",
    "        # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "        #\n",
    "        # Quem procurar: Otávio Augusto Iasbeck\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        @staticmethod\n",
    "        def teste_shapiro_wilks(df) -> pd.DataFrame:\n",
    "\n",
    "            '''\n",
    "                Parâmetro: \n",
    "                    1) df: Dataset com atributos\n",
    "\n",
    "                Retorno:\n",
    "                    1) Dataset com as colunas de input como valores, com atributos de p_value_shapiro_wilks, teste_estatistico_shapiro_wilks,\\n\n",
    "                        além de calculo de curtose e assimetria da distribuição para cada uma delas\n",
    "                \n",
    "                #### Observações:\n",
    "                    É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "\n",
    "            '''\n",
    "\n",
    "            ## Crio DataFrame com index no valor de cada coluna de df, com o valor da curtose e assimetra de cada coluna\n",
    "            distribuicao = pd.DataFrame({\n",
    "                         'Curtose': df.kurtosis(),\n",
    "                         'Assimetria': df.skew()\n",
    "                                })\n",
    "\n",
    "            lista_p_value = []\n",
    "            teste_estatistico = []\n",
    "\n",
    "            for coluna in df.columns:\n",
    "                try:\n",
    "                    ## Realizo o calculo de shapior para cada coluna\n",
    "                    valor_estatistico, p_value = scipy.stats.shapiro(df[coluna])\n",
    "\n",
    "                except Exception as erro:\n",
    "                    print(f\"Não pode ser realizado o calculo para a coluna {coluna}, provavelmente por tipagem de dados, segue erro: {erro}\")\n",
    "                    continue\n",
    "\n",
    "                ## Aloco em lista, cada valor\n",
    "                lista_p_value.append(np.around(p_value, 3))\n",
    "                teste_estatistico.append(valor_estatistico)\n",
    "                \n",
    "            ## Coloco no dataframe criado anteriormente\n",
    "            distribuicao['p_value_shapiro_wilks'] = lista_p_value\n",
    "            distribuicao['teste_estatistico_shapiro_wilks'] = teste_estatistico\n",
    "            \n",
    "            return distribuicao\n",
    "\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        # Função: testes_normalidade\n",
    "        #\n",
    "        # Descrição: Chama os testes que verificam normalidade através de testes de hipótese e calculo da distribuição\n",
    "        #\n",
    "        # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "        #\n",
    "        # Quem procurar: Otávio Augusto Iasbeck\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        def testes_normalidade(df) -> pd.DataFrame:\n",
    "\n",
    "            '''\n",
    "                Parâmetro: \n",
    "                    1) df: Dataset com atributos\n",
    "\n",
    "                Retorno:\n",
    "                    1) Dataset com as colunas de input como valores, com atributos de p_value_shapiro_wilks, teste_estatistico_shapiro_wilks\\n\n",
    "                        p_value_d_agostinho, teste_estatistico_d_agostinho para cada uma delas, além de calculo de curtose e assimetria da distribuição\n",
    "\n",
    "                #### Observações:\n",
    "                    É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "\n",
    "            '''\n",
    "            \n",
    "            ## Chamo as funções que realizam os testes\n",
    "            shapiro_wilks = Normalidade.Analisa_normalidade.teste_shapiro_wilks(df)\n",
    "            d_agostinho = Normalidade.Analisa_normalidade.teste_d_agostinho(df)\n",
    "                 \n",
    "            try:\n",
    "                ## Realizo o join pelo index das tabelas (que são as colunas)\n",
    "                df_all = pd.merge(\n",
    "                    left = shapiro_wilks,\n",
    "                    right = d_agostinho,\n",
    "                    how = 'left',\n",
    "                    on = shapiro_wilks.index\n",
    "                )\n",
    "                \n",
    "                ## Renomeio primeira coluna, que vem como default 'key_o'\n",
    "                df_all.rename(columns = {'key_0' : 'atributos'}, inplace = True)\n",
    "                \n",
    "                \n",
    "            except Exception as erro:\n",
    "                print(f\"Não foi possível realizar a junção dos testes, vide erro {erro}\")\n",
    "\n",
    "            return df_all \n",
    "    \n",
    "    \n",
    "\n",
    "    class Reajusta_dados:\n",
    "    \n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        # Função: padronizacao_z_score\n",
    "        #\n",
    "        # Descrição: Realiza a padronização por z_score que trata cada valor pela média e pela variança do conjutno\\\n",
    "        #            que o mesmo pertence\n",
    "        #\n",
    "        # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "        #\n",
    "        # Quem procurar: Otávio Augusto Iasbeck\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        @staticmethod\n",
    "        def padronizacao_z_score(df) -> pd.DataFrame:\n",
    "\n",
    "            '''\n",
    "                Parâmetro: \n",
    "                    1) df: Dataset com atributos \n",
    "\n",
    "                Retorno:\n",
    "                    1) Dataset com os valores transformados a partir da formula de Z_Score\n",
    "\n",
    "                #### Observações:\n",
    "                    É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "            '''\n",
    "\n",
    "            ## Crio dataframe\n",
    "            score_z = pd.DataFrame()\n",
    "            for col in df.columns:\n",
    "                try:\n",
    "                    ## Crio coluna com o nome da coluna original e coloco o sufixo '_SCORE_Z', alocando o valor do calculo\n",
    "                    score_z[f'{col}' + '_score_z'] = scipy.stats.zscore(df[col], nan_policy='omit')\n",
    "                except:\n",
    "                    ## Se não conseguir realizar o calculo, só aloca a coluna no dataframe\n",
    "                    score_z[f'{col}'] = list(df[f'{col}'])\n",
    "\n",
    "            return score_z\n",
    "\n",
    "\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        # Função: normalizacao_1_0\n",
    "        #\n",
    "        # Descrição: Realiza a normalização do dado baseado no seu conjunto, utilizando o máximo e o mínimo do conjunto\\\n",
    "        #            que o mesmo pertence\n",
    "        #\n",
    "        # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "        #\n",
    "        # Quem procurar: Otávio Augusto Iasbeck\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        @staticmethod\n",
    "        def normalizacao_1_0(df) -> pd.DataFrame:\n",
    "\n",
    "            '''\n",
    "                Parâmetro: \n",
    "                    1) df: Dataset com atributos \n",
    "\n",
    "                Retorno:\n",
    "                    1) Dataset com os valores transformados com relação ao seus Máximos e Mínimos\n",
    "\n",
    "                #### Observações:\n",
    "                    É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "            '''\n",
    "\n",
    "\n",
    "            ## Explicito o método\n",
    "            escala = MinMaxScaler()\n",
    "            ## Utilizo a transformação para os valores de df\n",
    "            array_normalizado = escala.fit_transform(df)\n",
    "\n",
    "            ## Crio dataframe com esses valores nomralizados\n",
    "            df_normalizado = pd.DataFrame(array_normalizado, \n",
    "                                      columns=df.columns,\n",
    "                                      index=df.index)\n",
    "            \n",
    "            [df_normalizado.rename(columns = {col : f'{col}_maxmin'}, inplace = True) for col in df_normalizado.columns]\n",
    "\n",
    "            return df_normalizado\n",
    "\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        # Função: normalizacao_log\n",
    "        #\n",
    "        # Descrição: Realiza a transformação do valor em log inverso na base 'e', ou seja log(e(exp(x))) , que é chamado de logarítimo natural\n",
    "        #\n",
    "        # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "        #\n",
    "        # Quem procurar: Otávio Augusto Iasbeck\n",
    "        # ---------------------------------------------------------------------------------------------------------- #\n",
    "        def transformacao_logaritimica(df) -> pd.DataFrame:\n",
    "\n",
    "            '''\n",
    "                Parâmetro: \n",
    "                    1) df: Dataset com atributos \n",
    "\n",
    "                Retorno:\n",
    "                    1) Dataset com os valores transformados para logarítimo\n",
    "                \n",
    "                #### Observações:\n",
    "                    É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "\n",
    "            '''\n",
    "\n",
    "            df_sqrt = pd.DataFrame()\n",
    "            for i in df.columns:\n",
    "                df_sqrt[f'{i}_log'] = np.log(df[i])\n",
    "\n",
    "            df_sqrt.dropna(axis = 1, inplace = True)\n",
    "            return df_sqrt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Varianca:\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    # Função: teste_levene\n",
    "    #\n",
    "    # Descrição: Realiza teste de levene para todas combinações de colunas\\\n",
    "    #            alocando o valor com p_valor > nível de significância em um dicionário\n",
    "    #\n",
    "    # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "    #\n",
    "    # Quem procurar: Otávio Augusto Iasbeck\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    @staticmethod\n",
    "    def teste_levene(df) -> pd.DataFrame: \n",
    "        \n",
    "        '''\n",
    "            Parâmetro: \n",
    "                1) df: Dataset com atributos de tipagem numérica (float, int)\n",
    "\n",
    "            Retorno:\n",
    "                1) Dataset com as colunas inputadas como valores de uma coluna, possuindo atributos de p_value_Levene e teste_estatistico_levene do teste de Levene\n",
    "            \n",
    "            #### Observações:\n",
    "                É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "        '''\n",
    "\n",
    "        relacoes = []\n",
    "        p_value_col = []\n",
    "        teste_estatistico = []\n",
    "\n",
    "        ## Percorro as colunas 2x\n",
    "        for coluna_x in df.columns:\n",
    "            for coluna_y in df.columns:\n",
    "                \n",
    "                ## Para não ocasionar em colunas iguais\n",
    "                if coluna_x == coluna_y:\n",
    "                    continue\n",
    "                    \n",
    "                ## Colocamos o parâmetro de center = 'mean' pois na formula podemos utilizar como variação dentro do calculo a mediana ou o média\\\n",
    "                ## e nos testes foi-se descoberto que a média trazia melhor adequação quanto à conjuntos não normais de distribuição\n",
    "                valor_estatistico, p_value = scipy.stats.levene(df[coluna_x], df[coluna_y])\n",
    "\n",
    "                \n",
    "                relacoes.append(f'{coluna_x} & {coluna_y}')\n",
    "                p_value_col.append(np.around(p_value, 4))\n",
    "                teste_estatistico.append(valor_estatistico)\n",
    "                \n",
    "        df_variancas = pd.DataFrame(data = {'p_value_Levene':p_value_col, 'atributos':relacoes, 'teste_estatistico_levene':teste_estatistico})\n",
    "        df_variancas = df_variancas.set_index('atributos') \n",
    "            \n",
    "        #df_variancas.drop_duplicates(subset = ['p_value_Levene', 'teste_estatistico_levene'], inplace = True)\n",
    "            \n",
    "        return df_variancas\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    # Função: teste_bartlett\n",
    "    #\n",
    "    # Descrição: Realiza teste de levene para todas combinações de colunas\\\n",
    "    #            alocando o valor com p_valor > nível de significância em um dicionário\n",
    "    #\n",
    "    # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "    #\n",
    "    # Quem procurar: Otávio Augusto Iasbeck\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    @staticmethod\n",
    "    def teste_bartlett(df) -> pd.DataFrame: \n",
    "\n",
    "        '''\n",
    "            Parâmetro: \n",
    "                1) df: Dataset com atributos de tipagem numérica (float, int)\n",
    "\n",
    "            Retorno:\n",
    "                1) Dataset com as colunas inputadas como valores de uma coluna, possuindo atributos de p_value_Bartlett e teste_estatistico_bartlett do teste de Bartlett\n",
    "\n",
    "            #### Observações:\n",
    "                É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "        '''\n",
    "\n",
    "        relacoes = []\n",
    "        p_value_col = []\n",
    "        teste_estatistico = []\n",
    "        \n",
    "        ## Percorro as colunas 2x\n",
    "        for coluna_x in df.columns:\n",
    "            for coluna_y in df.columns:\n",
    "                \n",
    "                ## Para não ocasionar em colunas iguais\n",
    "                if coluna_x == coluna_y:\n",
    "                    continue\n",
    "                \n",
    "                ## Realizo o calculo e trago somente o p_value\n",
    "                valor_estatistico, p_value = scipy.stats.bartlett(df[coluna_x], df[coluna_y])\n",
    "                \n",
    "                relacoes.append(f'{coluna_x} & {coluna_y}')\n",
    "                p_value_col.append(np.around(p_value, 4))\n",
    "                teste_estatistico.append(valor_estatistico)\n",
    "                \n",
    "        df_variancas = pd.DataFrame(data = {'p_value_Bartlett':p_value_col, 'atributos':relacoes, 'teste_estatistico_bartlett':teste_estatistico})\n",
    "        df_variancas = df_variancas.set_index('atributos')     \n",
    "        \n",
    "        #df_variancas.drop_duplicates(subset = ['p_value_Bartlett', 'teste_estatistico_bartlett'], inplace = True)\n",
    "        \n",
    "        return df_variancas\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    # Função: testes_variancas\n",
    "    #\n",
    "    # Descrição: Realiza a junção dos valores de varianças que foram obtidos realizando a combinação entre os atributos\\\n",
    "    #            posteriormente realizado join com index\n",
    "    #\n",
    "    # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "    #\n",
    "    # Quem procurar: Otávio Augusto Iasbeck\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    def testes_variancas(df) -> pd.DataFrame:\n",
    "\n",
    "        '''\n",
    "            Parâmetro: \n",
    "                1) df: Dataset com atributos de tipagem numérica (float, int)\n",
    "\n",
    "            Retorno:\n",
    "                1) Dataset com as colunas inputadas como valores de uma coluna, possuindo atributos de p_value e r_statics do teste de Levene e Bartlett\\\n",
    "            \n",
    "            #### Observações:\n",
    "                É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "        '''\n",
    "\n",
    "        ## Chamo as funções de testes \n",
    "        levene = Varianca.teste_levene(df)\n",
    "        bartlett = Varianca.teste_bartlett(df)\n",
    "        \n",
    "        try:\n",
    "            ## Realizo o join pelo index\n",
    "            testes_variancas = pd.merge(\n",
    "                left = levene,\n",
    "                right = bartlett,\n",
    "                how = 'left',\n",
    "                on = levene.index\n",
    "            )\n",
    "                \n",
    "            ## Renomeio primeira coluna, que vem como default 'key_o'\n",
    "            testes_variancas.rename(columns = {'key_0' : 'atributos'}, inplace = True)\n",
    "        \n",
    "        except Exception as erro:\n",
    "            print(f\"Não foi possível realizar a junção dos testes, vide erro {erro}\")\n",
    "                \n",
    "        return testes_variancas\n",
    "\n",
    "\n",
    "class Correlacao():\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    # Função: kmo_msa\n",
    "    #\n",
    "    # Descrição: Realiza o calculo de MSA que visa buscar a signficância explicativa de uma variável\\\n",
    "    #            enquanto o KMO trás um valor de significância geral dos atributos, visando a análise\\\n",
    "    #            de uma aplicação fatorial\n",
    "    #\n",
    "    # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "    #\n",
    "    # Quem procurar: Otávio Augusto Iasbeck\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    def kmo_msa(df):\n",
    "\n",
    "        '''\n",
    "            Parâmetro: \n",
    "                1) df: Dataset com atributos de tipagem numérica (float, int)\n",
    "\n",
    "            Retorno:\n",
    "                1) Dataset com as colunas inputadas como valores de uma coluna, possuindo um atributo com valor de MSA para cada coluna\\n\n",
    "                2) Valor de KMO \n",
    "\n",
    "            #### Observações:\n",
    "                É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            ## Crio dataframe com os valores de MSA para cada coluna, que estão como index\n",
    "            kmo_aplicado = pd.DataFrame(\n",
    "                            list(calculate_kmo(df)[0]),\n",
    "                            index=list(df.columns),\n",
    "                            columns = ['MSA']\n",
    "                           )\n",
    "            \n",
    "        except Exception as erro:\n",
    "            print(f\"Não foi possível aplicar o modelo, vide erro: {erro}\")\n",
    "        \n",
    "        return calculate_kmo(df)[1], kmo_aplicado\n",
    "        \n",
    "        \n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    # Função: teste_sperman\n",
    "    #\n",
    "    # Descrição: Realiza o teste de Spearman de 2 formas, a primeira ele realiza, aloca em um dataframe\\\n",
    "    #            junto às colunas combinatórias como index e colunas informando os valores de retorno do teste\\\n",
    "    #            Também realiza o teste através de outro método, alocando os coeficientes positivos e negativos\\\n",
    "    #            em dicionários\n",
    "    #\n",
    "    # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "    #\n",
    "    # Quem procurar: Otávio Augusto Iasbeck\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    def teste_sperman(df):\n",
    "        \n",
    "        '''\n",
    "            Parâmetro: \n",
    "                1) df: Dataset com atributos de tipagem numérica (float, int)\n",
    "\n",
    "            Retorno:\n",
    "                1) Dataset com as colunas inputadas como valores de uma coluna, possuindo atributos de p_value e r_statics do teste de Sperman\\n\n",
    "                2) Dicionários com valores de correlações positivas (calculado com módulo diferente do primeiro retorno)\\n\n",
    "                3) Dicionário com valores de correlações negativas (calculado com módulo diferente do primeiro retorno)\n",
    "\n",
    "            #### Observações:\n",
    "                É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "        '''\n",
    "\n",
    "        atributos_correlacionados_negativos = []\n",
    "        atributos_correlacionados_positivos = []\n",
    "\n",
    "        ## Faço o primeiro teste, através do módulo 'correlation', que nos retorna um dataframe de correlações combinatórias\n",
    "        corr = rs.correlation.corr_pair(df, method= 'spearman')\n",
    "        corr.rename(columns = {'N': 'nmr_valores_spearman',\n",
    "                              'p-value':'p_value_spearman',\n",
    "                              'r value':'r_value_spearman'}, inplace = True)\n",
    "        \n",
    "        ## Tratativa para trazer todas as colunas, menos a 'NS_REAL'\n",
    "        #colunas_a[:] = (x for x in df.columns if x != \"NS_REAL\")\n",
    "        \n",
    "        ## Excluo a última coluna (baseando-se que a última coluna é a classe, pela tratativa inicial dos dados)\n",
    "        atributos = df.drop(columns = {f'{str(df.columns[len(df.columns)-1:][0])}'})\n",
    "        atributos = df\n",
    "        \n",
    "        ## Percorro as colunas do df 2x\n",
    "        for coluna_x in atributos.columns:\n",
    "            for coluna_y in atributos.columns:\n",
    "                ## Aplico o método de spearman pelo módulo 'scipy.stats'\n",
    "                correlacao_value, valor_p = scipy.stats.spearmanr(atributos[coluna_x], atributos[coluna_y])\n",
    "                ## Se forem a mesma coluna, pula\n",
    "                if ((str(coluna_y) == str(coluna_x)) == False):\n",
    "                    ## Se obtiver um correlação significativa\n",
    "                    if (correlacao_value > 0.6) or (correlacao_value < -0.6):\n",
    "                        ## Se for uma correlação negativa\n",
    "                        if correlacao_value < 0:\n",
    "                            atributos_correlacionados_negativos.append(f\"{coluna_x} : {coluna_y} -> {correlacao_value} com valor-p {valor_p}\")\n",
    "                        else:\n",
    "                            atributos_correlacionados_positivos.append(f\"{coluna_x} : {coluna_y} -> {correlacao_value} com valor-p {valor_p}\")\n",
    "        \n",
    "        return corr, atributos_correlacionados_positivos, atributos_correlacionados_negativos\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    # Função: teste_pearson\n",
    "    #\n",
    "    # Descrição: Realiza o teste de Pearson de 2 formas, a primeira ele realiza, aloca em um dataframe\\\n",
    "    #            junto às colunas combinatórias como index e colunas informando os valores de retorno do teste\\\n",
    "    #            Também realiza o teste através de outro método, alocando os coeficientes positivos e negativos\\\n",
    "    #            em dicionários\n",
    "    #\n",
    "    # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "    #\n",
    "    # Quem procurar: Otávio Augusto Iasbeck\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    def teste_pearson(df):\n",
    "\n",
    "        '''\n",
    "            Parâmetro: \n",
    "                1) df: Dataset com atributos de tipagem numérica (float, int)\n",
    "\n",
    "            Retorno:\n",
    "                1) Dataset com as colunas inputadas como valores de uma coluna, possuindo atributos de p_value e r_statics do teste de Pearson\\n\n",
    "                2) Dicionários com valores de correlações positivas (calculado com módulo diferente do primeiro retorno)\\n\n",
    "                3) Dicionário com valores de correlações negativas (calculado com módulo diferente do primeiro retorno)\n",
    "\n",
    "            #### Observações:\n",
    "                É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "        '''\n",
    "\n",
    "        atributos_correlacionados_negativos = []\n",
    "        atributos_correlacionados_positivos = []\n",
    "\n",
    "        ## Faço o primeiro teste, através do módulo 'correlation', que nos retorna um dataframe de correlações combinatórias\n",
    "        corr = rs.correlation.corr_pair(df, method= 'pearson')\n",
    "        corr.rename(columns = {'N': 'nmr_valores_pearson',\n",
    "                              'p-value':'p_value_pearson',\n",
    "                              'r value':'r_value_pearson'}, inplace = True)\n",
    "        \n",
    "        ## Excluo a última coluna (baseando-se que a última coluna é a classe, pela tratativa inicial dos dados)\n",
    "        atributos = df.drop(columns = {f'{str(df.columns[len(df.columns)-1:][0])}'})\n",
    "        atributos = df\n",
    "        \n",
    "        ## Percorro as colunas do df 2x\n",
    "        for coluna_x in atributos.columns:\n",
    "            for coluna_y in atributos.columns:\n",
    "                ## Aplico o método de spearman pelo módulo 'scipy.stats'\n",
    "                correlacao_value, valor_p = scipy.stats.pearsonr(atributos[coluna_x], atributos[coluna_y])\n",
    "                ## Se forem a mesma coluna, pula\n",
    "                if ((str(coluna_y) == str(coluna_x)) == False):\n",
    "                    ## Se obtiver um correlação significativa\n",
    "                    if (correlacao_value > 0.6) or (correlacao_value < -0.6):\n",
    "                        ## Se for uma correlação negativa\n",
    "                        if correlacao_value < 0:\n",
    "                            atributos_correlacionados_negativos.append(f\"{coluna_x} : {coluna_y} -> {correlacao_value} com valor-p {valor_p}\")\n",
    "                        else:\n",
    "                            atributos_correlacionados_positivos.append(f\"{coluna_x} : {coluna_y} -> {correlacao_value} com valor-p {valor_p}\")\n",
    "        \n",
    "        return corr, atributos_correlacionados_positivos, atributos_correlacionados_negativos\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    # Função: testes_correlacoes\n",
    "    #\n",
    "    # Descrição: Realiza a junção dos valores de correlação que foram obtidos realizando a combinação entre os atributos\\\n",
    "    #            posteriormente realizado join com index\n",
    "    #\n",
    "    # Parâmetros: 1) dataframe = dataframe com dados formatados pela tipagem e por indicadores\n",
    "    #\n",
    "    # Quem procurar: Otávio Augusto Iasbeck\n",
    "    # ---------------------------------------------------------------------------------------------------------- #\n",
    "    def testes_correlacoes(df) -> pd.DataFrame:\n",
    "\n",
    "        '''\n",
    "            Parâmetro: \n",
    "                1) df: Dataset com atributos de tipagem numérica (float, int)\n",
    "\n",
    "            Retorno:\n",
    "                Dataset com as colunas inputadas como valores de uma coluna, possuindo atributos de p_value e r_statics do teste de Sperman e Pearson\n",
    "            \n",
    "            #### Observações:\n",
    "                É necessário que as colunas estejam em tipagem numérica como Float ou Int\n",
    "        '''\n",
    "\n",
    "        ## Chamo as funções de testes e pego o primeiro return de cada uma \n",
    "        sperman = Correlacao.teste_sperman(df)[0]\n",
    "        pearson = Correlacao.teste_pearson(df)[0]\n",
    "        \n",
    "        try:\n",
    "            ## Realizo o join pelo index das tabelas\n",
    "            testes_correlacoes = pd.merge(\n",
    "                left = sperman,\n",
    "                right = pearson,\n",
    "                how = 'left',\n",
    "                on = sperman.index\n",
    "            )\n",
    "                \n",
    "        except Exception as erro:\n",
    "            print(f\"Não foi possível realizar a junção dos testes, vide erro {erro}\")\n",
    "                \n",
    "        ## Renomeio primeira coluna, que vem como default 'key_o'\n",
    "        testes_correlacoes.rename(columns = {'key_0' : 'atributos'}, inplace = True)\n",
    "                \n",
    "        return testes_correlacoes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8525f-48f7-424d-90cb-4e18c3d7d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tratativa.conhecimento_base(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3b720-3660-4e28-b2b3-d4e4ae3ec297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new, df_continuo = Tratativa.higienizacao(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260325d9-daf3-41ae-baf2-9aa7c6455653",
   "metadata": {},
   "source": [
    "### OBS: Daqui em diante, foram feitos alguns códigos para auxílio na compilção desses testes e facilidade na interpretação baseando-se em regras de aceitação , porém cada função pode ser chamado individualmente!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe7437-c975-4a8a-a76e-feb249008522",
   "metadata": {},
   "source": [
    "## Verifica normalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee3e77-beb1-4fca-9e69-ad26fbf90b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalidade.Analisa_normalidade.distribuicao_grafica(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c401e-c827-4999-aafa-703ad3db17bb",
   "metadata": {},
   "source": [
    "### Criando regras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "026568b9-60f0-4817-9c93-342783c156cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "significancia_norm = 0.05\n",
    "porcentagem_base_aceitavel = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c3476d5-b511-4fff-a8d4-dc430fac6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regras_normalidade(significancia_norm, porcentagem_base_aceitavel, df):\n",
    "    \n",
    "    df_analise_normalidade = Normalidade.Analisa_normalidade.testes_normalidade(df)\n",
    "    ## Verificando por p_value\n",
    "    normalidade_aceitavel = df_analise_normalidade.loc[\n",
    "                                                        (df_analise_normalidade['p_value_shapiro_wilks'] > significancia_norm) |\\\n",
    "                                                        (df_analise_normalidade['p_value_d_agostinho'] > significancia_norm)\n",
    "                                                      ]\n",
    "    \n",
    "    if len(normalidade_aceitavel) >= round((len(df_analise_normalidade) * (porcentagem_base_aceitavel/100))):\n",
    "        print(f\"A base de dados possui {len(normalidade_aceitavel)} atributos ({list(normalidade_aceitavel['atributos'])}) que seguem uma distribuição normal\\\n",
    " Sendo assim, igual ou maior que {porcentagem_base_aceitavel}% da base de dados\")\n",
    "        aceitavel = 's'\n",
    "    \n",
    "    else:\n",
    "        print(f\"A base de dados possui {len(normalidade_aceitavel)} atributos ({list(normalidade_aceitavel['atributos'])}) que seguem uma distribuição normal\\n\\\n",
    "Não conseguindo atingir a quantidade aceitavel passada como parâmetro!({porcentagem_base_aceitavel}%).\\n\\\n",
    "Sugere-se um pré-processamento nos dados para prosseguir com a análise de correlação\")\n",
    "        aceitavel = 'n'\n",
    "        \n",
    "        return aceitavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259631d-8d2c-4d06-ad75-8cc817786d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "aceitavel = regras_normalidade(significancia_norm, porcentagem_base_aceitavel, df_continuo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe24b90-1ec8-4f83-863a-9dc95ffd87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if aceitavel == 'n':\n",
    "    print(\"Visto que a porcentagem de aceitação não foi atendida, será aplicado remodelagens de dados como normalização e padronização\\n\\\n",
    "    afim de conseguir um percentual aceitável de normalidade\")\n",
    "    print(\"\\n ----- Aplicando Padronização Z_Score--------\\n\")\n",
    "    df_z_score = Normalidade.Reajusta_dados.padronizacao_z_score(df_continuo)\n",
    "    \n",
    "    aceitavel = regras_normalidade(significancia_norm, porcentagem_base_aceitavel, df_z_score)\n",
    "    \n",
    "if aceitavel == 'n':\n",
    "    print(\"\\n ----- Aplicando Normalização MaxMin --------\\n\")\n",
    "    df_max_min = Normalidade.Reajusta_dados.normalizacao_1_0(df_continuo)\n",
    "\n",
    "    aceitavel = regras_normalidade(significancia_norm, porcentagem_base_aceitavel, df_max_min)\n",
    "\n",
    "if aceitavel == 'n':\n",
    "    print(\"\\n ----- Aplicando Transformacao Logarítimica --------\\n\")\n",
    "    df_log = Normalidade.Reajusta_dados.transformacao_logaritimica(df_continuo)\n",
    "\n",
    "    aceitavel = regras_normalidade(significancia_norm, porcentagem_base_aceitavel, df_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0e67e-91a4-4d45-a51c-fa46e4f2a6ca",
   "metadata": {},
   "source": [
    "## Verifica variança semelhante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dbe54f-d843-4001-a88d-97df2fbb8e67",
   "metadata": {},
   "source": [
    "### Criando regras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2f067a-4423-46e1-8f1f-2dae38b802f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "significancia_vari = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a7166d9-05e0-4c27-bb17-a82e81c2d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regras_variancias(significancia_vari, df):\n",
    "    \n",
    "    df_analise_varianca = Varianca.testes_variancas(df)\n",
    "        \n",
    "    ## Verificando por p_value\n",
    "    varianca_aceitavel = df_analise_varianca.loc[\n",
    "                                                        (df_analise_varianca['p_value_Levene'] >= significancia_vari) |\n",
    "                                                        (df_analise_varianca['p_value_Bartlett'] >= significancia_vari)\n",
    "                                                      ]\n",
    "    varianca_aceitavel.drop_duplicates(subset = ['p_value_Levene', 'teste_estatistico_levene', 'p_value_Bartlett', 'teste_estatistico_bartlett'], inplace = True)\n",
    "    \n",
    "    if len(varianca_aceitavel) > 0:\n",
    "        for i in range(len(varianca_aceitavel)):\n",
    "            print(\"------------\")\n",
    "            print(f\"Os atributos que possuem a mesma variança foram: {varianca_aceitavel['atributos'].iloc[i]}\\nCom correlação de {round(varianca_aceitavel['teste_estatistico_levene'].iloc[i], 3)} para teste de Levene e {round(varianca_aceitavel['teste_estatistico_bartlett'].iloc[i], 2)} para Bartlett\") \n",
    "    else:\n",
    "        print(f\"Não foram encontradas varianças entre as variáveis que passem no teste de significancia passada como parâmetro!\")\n",
    "    return df_analise_varianca, varianca_aceitavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f28b5-5b0a-4b34-bfdb-5d2032217c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_varianca_, varianca_aceitavel_ = regras_variancias(significancia_vari, df_continuo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27465532-9d16-45ec-97e0-87fc8d34da4d",
   "metadata": {},
   "source": [
    "## Verifica correlações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb5293-8871-4630-b7e1-29cf13bcbfd5",
   "metadata": {},
   "source": [
    "### Criando regras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6ab0b5cc-d6ba-4741-85a4-57e80384d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "significancia_sperman = 0.03\n",
    "significancia_pearson = 0.03\n",
    "msa_aceitavel = 0\n",
    "correlacao_positiva = 0.5\n",
    "correlacao_negativa = -0.5\n",
    "porcentagem_base_aceitavel = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b275bbb9-375e-4e31-a4e3-2e3cb017e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regras_correlacoes(significancia_sperman, significancia_pearson, msa_aceitavel, correlacao_positiva, correlacao_negativa, porcentagem_base_aceitavel, df):\n",
    "    \n",
    "    #df_analise_correlacao.set_index('atributos', inplace = True)\n",
    "    \n",
    "    df_analise_correlacao = Correlacao.testes_correlacoes(df)\n",
    "    \n",
    "    atr = [df_analise_correlacao.atributos]\n",
    "    df_analise_correlacao.drop(columns = {'atributos'}, inplace = True)\n",
    "    df_analise_correlacao = df_analise_correlacao.astype('float')\n",
    "    df_analise_correlacao['atributos'] = atr[0]\n",
    "    \n",
    "    \n",
    "    ## Verificando por p_value\n",
    "    correlacao_aceitavel = df_analise_correlacao.loc[\n",
    "        \n",
    "                                        ((df_analise_correlacao['p_value_spearman'] > significancia_sperman) |\n",
    "                                        (df_analise_correlacao['p_value_pearson'] > significancia_pearson)) \n",
    "                                        &\n",
    "                                        ((df_analise_correlacao['r_value_spearman'] > correlacao_positiva) |\n",
    "                                        (df_analise_correlacao['r_value_spearman'] <= correlacao_negativa))\n",
    "        \n",
    "                                                  ]\n",
    "    \n",
    "    kmo, correlacao_aceitavel_kmo = Correlacao.kmo_msa(df_ordinais)\n",
    "    \n",
    "    \n",
    "    correlacao_aceitavel_kmo = correlacao_aceitavel_kmo.loc[\n",
    "                                    (correlacao_aceitavel_kmo['MSA'] > msa_aceitavel)\n",
    "                                                ].sort_values(by = 'MSA', ascending = False)\n",
    "    \n",
    "    \n",
    "    if len(correlacao_aceitavel) != 0:\n",
    "        print(\"Temos as seguintes correlações:\")\n",
    "        \n",
    "        for row in range(len(correlacao_aceitavel)):    \n",
    "            ## Apenas ordeno os dados do maior para o menor\n",
    "            correlacao_aceitavel.sort_values(by = 'r_value_spearman', ascending = False, inplace = True)\n",
    "            \n",
    "            informa = f\"{correlacao_aceitavel['atributos'].iloc[row]} com correlação de {correlacao_aceitavel['r_value_spearman'].iloc[row]}\\n\"\n",
    "            print(informa)\n",
    "            \n",
    "    else:\n",
    "        print(\"Não houve correlação que tenha passado no limites de aceitação!\\n\")\n",
    "        \n",
    "        \n",
    "    for i in range(len(correlacao_aceitavel_kmo)):\n",
    "        if correlacao_aceitavel_kmo['MSA'].iloc[i] > 0:\n",
    "            \n",
    "            print(f\"O atributo {correlacao_aceitavel_kmo.index[i]} possui {round((correlacao_aceitavel_kmo['MSA'].iloc[i] * 100))}% de impacto na importância entre os indicadores para explicação dos outros indicadores da operação\")\n",
    "            #print(f'KMO = {kmo}')\n",
    "        \n",
    "    return correlacao_aceitavel_kmo, correlacao_aceitavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395a0f6-2029-4981-a1db-6dd06857b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacao_aceitavel_kmo, correlacao_aceitavel = regras_correlacoes(significancia_sperman, significancia_pearson, msa_aceitavel, correlacao_positiva, correlacao_negativa, porcentagem_base_aceitavel, df_ordinais)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
